# -*- coding: utf-8 -*-
"""Submission2sistem rekomendasi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15KTlWQThYGwLBYQYvMHIa76FQ3rGxtj-
"""

#upload file json

from google.colab import files
files.upload()

#membuat directory kaggle dan dataset
!mkdir ~/.kaggle
!mkdir datasets
#Buat direktori bernama kaggle dan salin file kaggle.json di sana.
!cp kaggle.json ~/.kaggle/
#Ubah hak akses file
!chmod 600 ~/.kaggle/kaggle.json
#list kaggle dataset
!kaggle datasets list

"""## Business Understanding
Para penyedia jasa aplikasi pemutaran film harus meningkatkan performa sistem rekomendasi untuk kepuasan para pelanggannya ketika menggunakan jasa aplikasi pemutaran film.
### Problem Statement
- Bagaimana cara merekomendasikan film yang disukai dan dapat diminati oleh pengguna dan dijadikan rekomendasi?
### Goal
- Membuat sistem rekomendasi film yang disukai oleh pengguna
### Solution Approach
Solusi yang saya ajukan yaitu dengan menggunakan 2 algoritma machine learning untuk sistem rekomendasi yaitu:
- Collaborative Filtering adalah algoritma yang bergantung pada pendapat komunitas pengguna. Dia tidak memerlukan atribut untuk setiap itemnya. Algoritma ini memberikan rekomendasi berdasarkan nilai rating atau nilai lain, disini saya menggunakan target sebagai dasar penilaian[2].
- Content Based Filtering adalah algoritma yang merekomendasikan item serupa dengan apa yang disukai pengguna, berdasarkan tindakan mereka sebelumnya atau umpan balik eksplisit. Algoritma ini memberikan rekomendasi berdasarkan aktivitas pada masa lalu[3].
"""

#download kaggle dataset
!kaggle datasets download -d rohan4050/movie-recommendation-data

#unzip dataset yang telah didownload
!unzip /content/movie-recommendation-data -d datasets/

"""## Data Understanding
Data yang digunakan adalah data yang ada pada kaggle [Movie Recommendation Data](https://www.kaggle.com/datasets/rohan4050/movie-recommendation-data)

Dataset memiliki 4 *files* csv yang kita gunakan untuk sistem rekomendasi adalah:
 - movies.csv :  File Metadata Film utama. Berisi informasi tentang 9000 film yang ditampilkan dalam kumpulan data Full MovieLens. Fitur termasuk poster, latar belakang, anggaran, pendapatan, tanggal rilis, bahasa, negara produksi, dan perusahaan.
 - links.csv : Berisi ID TMDB (The Movie Database) dan IMDB (Internet Movie Database) dari sebagian kecil dari 9.000 film dari Kumpulan Data Lengkap.
 - ratings.csv : Sub kumpulan 100.000 peringkat dari 700 pengguna di 9.000 film.
 - tags.csv : berisi label untuk film



 Keterangan kolom:
- genre - merupakan ragam jenis film
- tmbdId - merupakan TMBD id
- imbdId - merupakan IMBD id
- title - merupakan judul film
- movieId - TMBD id
- rating - nilai rating
- tag - label film
"""

#membaca semua file yang telah didownload
import pandas as pd
 
link = pd.read_csv('/content/datasets/ml-latest-small/links.csv')
movie = pd.read_csv('/content/datasets/ml-latest-small/movies.csv')
rating = pd.read_csv('/content/datasets/ml-latest-small/ratings.csv')
tag = pd.read_csv('/content/datasets/ml-latest-small/tags.csv')

print('Jumlah data link movie : ', len(link.movieId.unique()))
print('Jumlah data movie : ', len(movie.movieId.unique()))
print('Jumlah data ratings dari user : ', len(rating.userId.unique()))
print('Jumlah data ratings dari user : ', len(rating.movieId.unique()))
print('Jumlah data : ', len(tag.movieId.unique()))

#melihat informasi pada link
link.info()

#melihat informasi pada movie
movie.info()

#melihat isi dari rating
rating.head()

#melihat isi dari movie
movie.head(5)

"""## Data Preparation
Teknik Data Preperation yang digunakan adalah:
-  menggabungkan isi dari semua file menjadi satu buah tabel output yang berisi userId,movieId,ratings,timestamp,title,genres.
- menkonversi beberapa data menjadi bentuk list dan dictonary.
- TrainTestSplit() untuk membagi dataset menjadi data latih (train) dan data uji (test) merupakan hal yang harus dilakukan sebelum membuat model. Mempertahankan sebagian data yang ada untuk menguji seberapa baik generalisasi model terhadap data baru. nilai yang digunakan untuk test adalah 0.2 atau 20% sehingga nilai yang digunakan untuk train adalah 0.8 atau 80% sehingga perbandingan rasio train/test adalah 80:20.
"""

#import library
import numpy as np
 
# Menggabungkan seluruh movieID pada kategori movie
movie_all = np.concatenate((
    link.movieId.unique(),
    movie.movieId.unique(),
    rating.movieId.unique(),
    tag.movieId.unique(),
))
 
# Mengurutkan data dan menghapus data yang sama
movie_all = np.sort(np.unique(movie_all))
 
print('Jumlah seluruh data movie berdasarkan movieID: ', len(movie_all))

# Menggabungkan seluruh userId
user_all = np.concatenate((
    rating.userId.unique(),
    tag.userId.unique(),
   
))
 
# Menghapus data yang sama kemudian mengurutkannya
user_all = np.sort(np.unique(user_all)) 
 
print('Jumlah seluruh user: ', len(user_all))

#menampilkan data yang null
movie.isnull().sum()

#mengelompokan movieid
movie.groupby('movieId').sum()

#memberikan variable baru untuk rating
semua_rate = rating
semua_rate

#menggabungkan isi dari movie id dan rating
nama_film = pd.merge(semua_rate, movie[['movieId','title','genres']], on='movieId', how='left')
nama_film

# Menggabungkan dataframe genres dengan nama_film dan memasukkannya ke dalam variabel semua_film
semua_film = pd.merge(nama_film, tag[['movieId','tag']], on='movieId', how='left')
semua_film

#membuang data yang cacat
semua_film_clean = semua_film.dropna()
semua_film_clean

#menyusun nilai dari movieid
fix_movie = semua_film_clean.sort_values('movieId', ascending=True)
fix_movie

#melihat jumlah data unik
len(fix_movie.movieId.unique())

#menyusun movieid
persiapan = fix_movie
persiapan.sort_values('movieId')

# Membuang data duplikat pada variabel persiapan
persiapan = persiapan.drop_duplicates('movieId')
persiapan

# Mengonversi data series ‘movieId’ menjadi dalam bentuk list
movie_id = persiapan['movieId'].tolist()
 
# Mengonversi data series ‘title’ menjadi dalam bentuk list
movie_name = persiapan['title'].tolist()
 
# Mengonversi data series ‘genres’ menjadi dalam bentuk list
movie_genre = persiapan['genres'].tolist()
 
print(len(movie_id))
print(len(movie_name))
print(len(movie_genre))

# Membuat dictionary untuk data ‘movie_id’, ‘movie_name’, dan ‘movie_genre’
movie_new = pd.DataFrame({
    'id': movie_id,
    'movie_name': movie_name,
    'genre': movie_genre
})
movie_new

"""## Modeling
### Content Based Filtering
- Content based filtering menggunakan informasi tentang beberapa item/data untuk merekomendasikan kepada pengguna sebagai referensi mengenai informasi yang digunakan sebelumnya. Tujuan dari content based filtering adalah untuk memprediksi persamaan sejumlah informasi yang didapat dari pengguna. Sebagai contoh, seorang penikmat film sedang melihat film bergenre drama. Platform film online secara sistem akan merekomendasikan si pengguna untuk memperlihatkan film lain yang berhubungan dengan drama. Dalam pembuatannya, content based filtering menggunakan konsep perhitungan Cosine Similarity yang intinya mengukur kesamaan antara dua vektor dan menentukan apakah kedua vektor tersebut menunjuk ke arah yang sama.

- Cosine similarity mengukur kesamaan antara dua vektor dan menentukan apakah kedua vektor tersebut menunjuk ke arah yang sama. Ia menghitung sudut cosinus antara dua vektor. Semakin kecil sudut cosinus, semakin besar nilai cosine similarity.dalah metrik yang digunakan untuk menentukan seberapa mirip dokumen terlepas dari ukurannya.
Secara matematis, Cosine similarity mengukur kosinus sudut antara dua vektor yang diproyeksikan dalam ruang multidimensi.
Dalam konteks ini, dua vektor yang saya bicarakan adalah larik yang berisi jumlah kata dari dua dokumen. Untuk mendefinisikannya, barisan dipandang sebagai vektor dalam ruang hasil kali dalam, dan kesamaan kosinus didefinisikan sebagai cosinus sudut di antara mereka, yaitu perkalian titik vektor dibagi dengan perkalian panjangnya. Oleh karena itu kesamaan kosinus tidak bergantung pada besaran vektor, tetapi hanya pada sudutnya. Kemiripan cosinus selalu termasuk dalam interval [-1,1].Misalnya, dua vektor proporsional memiliki kesamaan kosinus 1, dua vektor ortogonal memiliki kesamaan 0, dan dua vektor berlawanan memiliki kesamaan -1. Kesamaan cosinus terutama digunakan dalam ruang positif, di mana hasilnya dibatasi dengan rapi di [0,1].
- TF-IDF (term frequency-inverse document frequency) adalah ukuran statistik yang mengevaluasi seberapa relevan suatu kata dengan dokumen dalam kumpulan dokumen.
Hal ini dilakukan dengan mengalikan dua metrik: berapa kali sebuah kata muncul dalam dokumen, dan frekuensi dokumen kebalikan dari kata tersebut di seluruh kumpulan dokumen.TFIDF bekerja dengan meningkatkan secara proporsional berapa kali sebuah kata muncul dalam dokumen tetapi diimbangi dengan jumlah dokumen yang menampilkannya. Oleh karena itu, kata-kata seperti 'ini', 'adalah' dll, yang biasanya ada di semua dokumen tidak diberi peringkat yang sangat tinggi. Namun, sebuah kata yang muncul terlalu sering dalam beberapa dokumen akan diberikan peringkat yang lebih tinggi karena dapat menunjukkan konteks dari dokumen tersebut.
#### Keuntungan Content Based Filtering
- Model tidak memerlukan data tentang pengguna lain, karena rekomendasi bersifat khusus untuk pengguna ini. Hal ini mempermudah penskalaan ke sejumlah besar pengguna.
- Model ini dapat menangkap minat spesifik pengguna, dan dapat merekomendasikan item khusus yang sangat diminati oleh sedikit pengguna lain
#### Kekurangan Content Based Filtering
- Karena representasi fitur item dirancang secara manual hingga tingkat ini, teknik ini memerlukan banyak pengetahuan domain. Oleh karena itu, model hanya bisa sebaik fitur yang dirancang dengan tangan
- Model hanya dapat membuat rekomendasi berdasarkan minat pengguna yang ada. Dengan kata lain, model memiliki kemampuan terbatas untuk memperluas minat pengguna yang ada
"""

from sklearn.feature_extraction.text import TfidfVectorizer
 
# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data genre
tf.fit(movie_new['genre']) 
 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

#Pelajari kosakata dan idf, kembalikan matriks istilah dokumen.
tfidf_matrix = tf.fit_transform(movie_new['genre']) 
tfidf_matrix.shape

#convert to dense
tfidf_matrix.todense()

#menampilkan data frame
pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=movie_new.movie_name
).sample(22, axis=1).sample(10, axis=0)

#menghitung nilai cosine
from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

#menampilkan dataframe cosine
cosine_sim_df = pd.DataFrame(cosine_sim, index=movie_new['movie_name'], columns=movie_new['movie_name'])
print('Shape:', cosine_sim_df.shape)
 
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def movie_recommendations(nama_movie, similarity_data=cosine_sim_df, items=movie_new[['movie_name', 'genre']], k=5):
   
 
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan    
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_movie].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop nama_movie agar nama movie yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_movie, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

#mencari rekomendasi yang mirip
movie_new[movie_new.movie_name.eq('Grumpier Old Men (1995)')]

#hasil rekomendasi
movie_recommendations('Grumpier Old Men (1995)')

"""## Modeling
### Collorative Filtering
Metode Colaborative filtering merupakan metode yang melakukan proses penyaringan item yang berdasarkan pengguna lain, dengan cara memberikan informasi kepada pengguna berdasarkan kemiripan karakteristik. Dalam pembuatanya saya menggunakan RecommenderNet, pada tahap ini model menghitung skor kecocokan antara pengguna dan film dengan teknik embedding. Pertama, kita melakukan proses embedding terhadap data user dan film. Selanjutnya, lakukan operasi perkalian dot product antara embedding user dan film. Selain itu, kita juga dapat menambahkan bias untuk setiap user dan film. Skor kecocokan ditetapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid. Lapisan embedding  adalah salah satu lapisan yang tersedia di Keras. Ini terutama digunakan dalam aplikasi terkait Pemrosesan Bahasa Alami seperti pemodelan bahasa, tetapi juga dapat digunakan dengan tugas lain yang melibatkan jaringan saraf. Saat menangani masalah NLP, kita dapat menggunakan embedding kata yang telah dilatih sebelumnya seperti GloVe. Alternatifnya kita juga bisa melatih embedding kita sendiri menggunakan lapisan embedding Keras.Metode ini menggunakan Binary Crossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation.
#### Kelebihan
- Kita tidak memerlukan pengetahuan domain karena penyematan dipelajari secara otomatis.
- Model dapat membantu pengguna menemukan minat baru. Secara terpisah, sistem ML mungkin tidak tahu apakah pengguna tertarik dengan item tertentu, tetapi model mungkin masih merekomendasikannya karena pengguna serupa tertarik pada item tersebut.
- Sampai batas tertentu, sistem hanya memerlukan matriks masukan untuk melatih model faktorisasi matriks. Secara khusus, sistem tidak memerlukan fitur kontekstual. Dalam praktiknya, hal ini dapat digunakan sebagai salah satu dari beberapa generator kandidat.
#### Kekurangan
- Prediksi model untuk pasangan (pengguna, item) tertentu adalah produk titik dari penyematan yang sesuai. Jadi, jika item tidak terlihat selama pelatihan, sistem tidak dapat membuat penyematan untuk item tersebut dan tidak dapat melakukan kueri model dengan item ini. Masalah ini sering disebut masalah cold start
- Fitur samping adalah setiap fitur di luar kueri atau ID item. Untuk rekomendasi film, fitur samping mungkin menyertakan negara atau usia. Menyertakan fitur samping yang tersedia akan meningkatkan kualitas model. Meskipun mungkin tidak mudah untuk menyertakan fitur samping di WALS, generalisasi WALS memungkinkan hal ini.
"""

#import library
import pandas as pd
import numpy as np 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

#menampilkan file rating
df = rating
df

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['userId'].unique().tolist()
print('list userID: ', user_ids)
 
# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah movieId menjadi list tanpa nilai yang sama
movie_ids = df['movieId'].unique().tolist()
 
# Melakukan proses encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
 
# Melakukan proses encoding angka ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}
 
# Selanjutnya, petakan userId dan movieId ke dataframe yang berkaitan.
 
# Mapping userId ke dataframe genres
df['genres'] = df['userId'].map(user_to_user_encoded)
 
# Mapping movieD ke dataframe movies
df['movies'] = df['movieId'].map(movie_to_movie_encoded)

num_users = len(user_to_user_encoded)
print(num_users)
 
num_movie = len(movie_encoded_to_movie)
print(num_movie)
 
df['ratings'] = df['rating'].values.astype(np.float32)
 
min_rating = min(df['rating'])
 
max_rating = max(df['rating'])
 
print('Number of User: {}, Number of movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data genres  dan movies menjadi satu value
x = df[['genres', 'movies']].values
 
# Membuat variabel y untuk membuat ratings dari hasil 
y = df['ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movies
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movies bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2) 
 
    x = dot_user_movie + user_bias + movie_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 50,
    validation_data = (x_val, y_val)
)

#visualisasi model
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#mendapatkan rekomendasi film
movie_df = movie_new
df = pd.read_csv('/content/datasets/ml-latest-small/ratings.csv')
 

user_id = df.userId.sample(1).iloc[0]
movie_watched_by_user = df[df.userId == user_id]
 

movie_not_watched = movie_df[~movie_df['id'].isin(movie_watched_by_user.movieId.values)]['id'] 
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)
 
movie_not_watched = [[movie_to_movie_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched))

#menampilkan hasil rekomendasi
ratings = model.predict(user_movie_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('movie with high ratings from user')
print('----' * 8)
 
top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)
 
movie_df_rows = movie_df[movie_df['id'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.movie_name, ':', row.genre)
 
print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)
 
recommended_movie = movie_df[movie_df['id'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.movie_name, ':', row.genre)